\chapter{Large Language Models}\label{ch:techOverview}

Creating machines with human-like language understanding has been a subject of research since the 1950s.
\gls{natural-language}s are highly complex and pose a difficult challenge to computers.
\gls{lm} is an approach to make machines read, write and communicate like humans~\autocite{zhao2023survey}.

The main idea of \gls{lm} is to model probabilities of

language modeling approach: text as a sequence of words -> model generative likelihood
-> predict probability of next tokens

popular in the 1990s: statistical language models


from~\autocite{jozefowicz2016exploring}:
language modeling has found use in many \gls{nlp} tasks
speech recognition, machine translation, text summarization

probability distribution over words~\autocite{de2015survey}




\section{Deep Neural Networks}\label{sec:dnn}

\subsection{Multi Layer Perceptron}\label{subsec:multi-layer-perceptron}

\subsection{Learning from Data (Gradient Descent)}\label{subsec:learning-from-data}
% Mathematical Optimization

\subsubsection{Loss Functions}

\subsubsection{Backpropagation}

\section{Transformers}\label{sec:trans}

\section{Decoder-only Models}\label{sec:decoder}

\subsection{GPT}\label{subsec:gpt}

\subsection{Llama}\label{subsec:llama}

\subsubsection{Leo}

\section{Supervised Fine-Tuning (SFT)}\label{sec:supervised-fine-tuning}

\section{Alignment Methods}\label{sec:alignment-methods}

\subsection{RLHF}\label{subsec:rlhf}
\subsection{PPO}\label{subsec:ppo}
\subsection{DPO}\label{subsec:dpo}
